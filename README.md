# ðŸ§  HPC-LM: Domain-Specific LLM for High-Performance Computing

HPC-LM is an end-to-end playground for building a compact GPT-style language model specialized in high-performance computing (HPC). The repo covers dataset creation, tokenizer training, transformer implementation, and training/testing utilities so you can reproduce the full pipeline from raw documents to a working model.

## Project Scope
- Gather HPC lecture slides, manuals, and PDFs, then clean them into a reusable text corpus.
- Train a SentencePiece BPE tokenizer tailored to HPC terminology such as OpenMP, NUMA, and vectorization.
- Implement a GPT architecture with transformer layers, rotary position encodings, and KV caching.
- Provide training and evaluation scripts plus simple tests to validate the tokenizer and data pipeline.

## Directory Overview
- `data/corpus/` â€“ cleaned text corpora; each file is timestamped (e.g., `hpc_corpus_20251028_133206.txt`).
- `data/tokenizer/` â€“ tokenizer artifacts produced by the training script (`.model` and `.vocab`).
- `src/data_extractor/` â€“ utilities for scraping, parsing, and normalizing HPC documents.
- `src/tokenizer/` â€“ tokenizer trainer (`tokenizer_train.py`) and the runtime `TokenizerWrapper`.
- `src/llm/` â€“ GPT implementation:
  - `gpt.py` holds the model definition.
  - `layers.py`, `rope.py`, and `kvcache.py` implement attention blocks, rotary embeddings, and caching.
- `src/train/train.py` â€“ command-line training loop; defines the training environment, optimizer, sampling, and checkpoint logic.
- `src/test/` â€“ lightweight tests (e.g., `tokenizer_wrapper_test.py`).
- `runs/` â€“ default output directory for checkpoints and logs generated by `train.py`.
- `test.py` â€“ smoke-test script for loading a trained model and generating text.

## Tooling and Environments
- **Dependency manager:** [`uv`](https://github.com/astral-sh/uv). Use `uv sync` to materialize the Python environment specified in `pyproject.toml`.
- **Training environment:** `src/train/train.py` orchestrates model training. It instantiates datasets, builds the GPT model from `src/llm/gpt.py`, and handles AMP, gradient clipping, evaluation, and sampling.
- **Test environment:** `src/test/tokenizer_wrapper_test.py` ensures the tokenizer wrapper produces consistent token IDs, padding masks, and decodes back to readable text.

## Running the Pipeline
1. **Install dependencies**
   ```bash
   uv sync
   ```

2. **Train the tokenizer**
   ```bash
   uv run python src/tokenizer/tokenizer_train.py
   ```
   The SentencePiece model and vocabulary are written to `data/tokenizer/`.

3. **Train the GPT model**
   ```bash
   PYTHONPATH=src uv run python src/train/train.py \
     --data data/corpus/hpc_corpus_YYYYMMDD_HHMMSS.txt \
     --tok_model data/tokenizer/hpc_bpe.model \
     --out_dir runs/hpc_baseline \
     --max_seq_len 256 --batch_size 32 --n_layer 6 --n_head 8 --d_model 512
   ```
   Replace the corpus filename with the timestamped file you generated and adjust CLI flags to match your experiment (learning rate, number of steps, sampling frequency, etc.).

4. **Run tests**
   ```bash
   PYTHONPATH=src uv run python src/test/tokenizer_wrapper_test.py
   ```
   This confirms the tokenizer can encode/decode batches and produce attention masks required by the training loop.

## Generated Artifacts
- **Tokenization assets:** Stored under `data/tokenizer/` and reusable across training runs.
- **Model checkpoints:** Saved to `runs/<experiment>/` (`model_best.pt`, `model_final.pt`) with minimal config metadata.
- **Sample generations:** Printed during training by `train.py` to monitor model quality on-the-fly.
