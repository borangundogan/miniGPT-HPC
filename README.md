# ðŸ§  HPC-LM: Domain-Specific LLM for High-Performance Computing

HPC-LM is an end-to-end playground for building a compact GPT-style language model tailored to high-performance computing (HPC). It covers the full pipeline: collecting domain documents, producing a tokenizer, implementing the transformer, and training/evaluating the resulting model.

## Highlights
- Turn raw HPC lecture notes, manuals, and PDFs into a clean text corpus.
- Train a SentencePiece BPE tokenizer tuned for HPC jargon (OpenMP, NUMA, vectorization, ...).
- Build and train a GPT architecture with rotary positional embeddings and KV caching.
- Validate the stack with lightweight unit tests and simple smoke runs.

## Prerequisites
- Python 3.10+ (managed via [`uv`](https://github.com/astral-sh/uv)).
- Populated `data/corpus/` directory with timestamped text files (see pipeline below).
- Compute with CUDA if you plan to train larger models (not required for running scripts).

## Quickstart
1. **Install dependencies**
   ```bash
   uv sync
   ```

2. **Extract and normalize text**
   ```bash
   PYTHONPATH=src uv run python src/data_extractor/data_extractor.py
   PYTHONPATH=src uv run python src/data_extractor/corpus_prepare.py
   uv run python src/data_extractor/shuffle_corpus.py --input data/corpus/hpc_corpus_YYYYMMDD_HHMMSS.txt
   uv run python src/data_extractor/split_corpus.py --input data/corpus/hpc_corpus_YYYYMMDD_HHMMSS_shuffled.txt
   ```
   Replace the timestamp placeholders with the files generated in the previous step. The scripts produce train/test/eval splits under `data/corpus/`.

3. **Train the tokenizer**
   ```bash
   uv run python src/tokenizer/tokenizer_train.py
   ```
   SentencePiece artifacts (`.model`, `.vocab`) are written to `data/tokenizer/`.

4. **Launch training**
   ```bash
   PYTHONPATH=src uv run python src/train/train.py \
     --data data/corpus/hpc_corpus_YYYYMMDD_HHMMSS_shuffled.txt \
     --tok_model data/tokenizer/hpc_bpe.model \
     --out_dir runs/hpc_baseline \
     --max_seq_len 256 --batch_size 32 --n_layer 6 --n_head 8 --d_model 512
   ```
   Adjust CLI flags to match your experiment: learning rate, number of steps, sampling cadence, etc.

5. **Run the smoke test**
   ```bash
   PYTHONPATH=src uv run python src/test/tokenizer_wrapper_test.py
   ```
   Confirms that encoding/decoding and attention masks align with what the training loop expects.

6. **Generate text from a checkpoint**
   ```bash
   PYTHONPATH=src uv run python test.py \
     --ckpt runs/hpc_baseline/model_best.pt \
     --tok_model data/tokenizer/hpc_bpe_large_clean.model \
     --prompt "In modern supercomputers, the OpenMP parallel region" \
     --max_new_tokens 80
   ```
   Swap `--ckpt`, `--tok_model`, and the prompt to match your experiment. Optional flags `--temperature`, `--top_p`, and `--top_k` control sampling.

## Troubleshooting
- **Null characters in corpus:** clean the offending file before downstream steps:
  ```bash
  cat data/corpus/hpc_corpus_20251028_204036_shuffled.txt | tr -d '\000-\010\013\014\016-\037' > data/corpus/hpc_corpus_clean.txt
  ```

## Directory Layout
- `data/corpus/` â€“ cleaned corpora (timestamped text files and splits).
- `data/tokenizer/` â€“ tokenizer artifacts generated by `tokenizer_train.py`.
- `src/data_extractor/` â€“ scrapers, parsers, and corpus preparation utilities.
- `src/tokenizer/` â€“ tokenizer trainer (`tokenizer_train.py`) and runtime wrapper.
- `src/llm/` â€“ core model implementation (`gpt.py`, attention layers, rotary embeddings, KV cache).
- `src/train/train.py` â€“ CLI for model training, evaluation, and checkpointing.
- `src/test/` â€“ unit tests such as `tokenizer_wrapper_test.py`.
- `runs/` â€“ default output directory for checkpoints, logs, and sample generations.
- `test.py` â€“ quick script to load a checkpoint and generate text.

## Generated Artifacts
- **Tokenizer assets:** cached under `data/tokenizer/` and reusable across runs.
- **Model checkpoints:** stored in `runs/<experiment>/` (`model_best.pt`, `model_final.pt`) with minimal metadata.
- **Sample generations:** emitted during training to monitor model quality in real time.
